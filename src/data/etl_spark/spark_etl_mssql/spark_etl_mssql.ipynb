{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70802ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf  \n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "sys.path.append('/mnt/d/local-repo-github/enr_portfolio_modeling/')\n",
    "os.chdir('D:/local-repo-github/enr_portfolio_modeling/')\n",
    "spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('blx-mdp').getOrCreate()\n",
    "\n",
    "#Load Config\n",
    "config_file=os.path.join(os.path.dirname('__file__'), 'Config/config.ini') \n",
    "config=configparser.ConfigParser(allow_no_value=True)\n",
    "config.read(config_file)\n",
    "\n",
    "\n",
    "\n",
    "JAVA_HOME=os.path.join(os.path.dirname(\"__file__\"),config['develop']['JAVA_HOME'])\n",
    "# set Java home\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "pguid=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pguid'])\n",
    "pgserver=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pgserver'])\n",
    "pgdwhdb=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pgdwhdb'])\n",
    "pgport=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pgport'])\n",
    "pgpw=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pgpw'])\n",
    "src_driver=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pgdriver'])\n",
    "dest_driver=os.path.join(os.path.dirname(\"__file__\"),config['develop']['pgdriver'])\n",
    "\n",
    "properties = {\n",
    "    'user': pguid,\n",
    "    'password': pgpw,\n",
    "    'driver': src_driver\n",
    "}\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"blx-mdp\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\", \"/mnt/d/apache-spark/spark-3.3.2-bin-hadoop3/jars/postgresql-42.7.1.jar\")\n",
    "\n",
    "# source connection\n",
    "#src_url = f\"jdbc:sqlserver://{server}:1433;databaseName={src_db};user={uid};password={pwd};\"\n",
    "src_url = f\"jdbc:postgresql://{pgserver}:5432/{pgdwhdb}?user={pguid}&password={pgpw}\"\n",
    "# target connection\n",
    "target_url = f\"jdbc:postgresql://{pgserver}:5432/{pgdwhdb}?user={pguid}&password={pgpw}\"\n",
    "\n",
    "#init spark context\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "etl = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43485312",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o302.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\05\njava.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\05\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\05\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-f073ecaabac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stagging.\"ContractPrices\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o302.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\05\njava.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\05\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\05\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(src_url, 'stagging.\"ContractPrices\"', properties=properties)\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cc16960",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select  t.table_name as table_name from information_schema.tables t \n",
    "where t.table_schema = 'stagging' AND t.table_type = 'BASE TABLE'\n",
    "AND t.table_name in ('ContractPrices','ProductionAsset','VolumeHedge', 'MarketPrices')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab0875f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select * from stagging.\"ContractPrices\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e8b8157",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o271.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\07\njava.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\07\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\07\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-a065cc3e2cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's test our connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jdbc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_driver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpguid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpgpw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apache-spark-dir\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o271.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\07\njava.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\07\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.nio.file.NoSuchFileException: C:\\Users\\nherm\\AppData\\Local\\Temp\\blockmgr-224603cc-bdb2-4450-af24-26fff20ec2fa\\07\r\n\tat sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)\r\n\tat sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)\r\n\tat sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:504)\r\n\tat java.nio.file.Files.createDirectory(Files.java:674)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:131)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1996)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1477)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1369)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1858)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:135)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1513)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n"
     ]
    }
   ],
   "source": [
    "# Let's test our connection\n",
    "dfs=etl.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    options(driver=src_driver, user=pguid, password=pgpw, url=src_url, query=sql). \\\n",
    "    load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c377db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    try:\n",
    "        dfs=etl.read. \\\n",
    "            format(\"jdbc\"). \\\n",
    "            options(driver=src_driver,user=uid, password=pwd,url=src_url,query=sql). \\\n",
    "            load()\n",
    "        # get table names\n",
    "        data_collect = dfs.collect()\n",
    "        # looping thorough each row of the dataframe\n",
    "        for row in data_collect:\n",
    "        # while looping through each\n",
    "            print(row[\"table_name\"])\n",
    "            tbl_name = row[\"table_name\"]\n",
    "            df = etl.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"driver\", src_driver) \\\n",
    "            .option(\"user\", uid) \\\n",
    "            .option(\"password\", pwd) \\\n",
    "            .option(\"url\", src_url) \\\n",
    "            .option(\"dbtable\", f\"dbo.{tbl_name}\") \\\n",
    "            .load()\n",
    "            #print(df.show(10))\n",
    "            load(df, tbl_name)\n",
    "            print(\"Data loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Data extract error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    try:\n",
    "        dfs=etl.read. \\\n",
    "            format(\"jdbc\"). \\\n",
    "            options(driver=src_driver,user=uid, password=pwd,url=src_url,query=sql). \\\n",
    "            load()\n",
    "        # get table names\n",
    "        data_collect = dfs.collect()\n",
    "        # looping thorough each row of the dataframe\n",
    "        for row in data_collect:\n",
    "        # while looping through each\n",
    "            print(row[\"table_name\"])\n",
    "            tbl_name = row[\"table_name\"]\n",
    "            df = etl.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"driver\", src_driver) \\\n",
    "            .option(\"user\", uid) \\\n",
    "            .option(\"password\", pwd) \\\n",
    "            .option(\"url\", src_url) \\\n",
    "            .option(\"dbtable\", f\"dbo.{tbl_name}\") \\\n",
    "            .load()\n",
    "            #print(df.show(10))\n",
    "            load(df, tbl_name)\n",
    "            print(\"Data loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Data extract error: \" + str(e))\n",
    "        \n",
    "        \n",
    "def load(df, tbl):\n",
    "    try:\n",
    "        rows_imported = 0\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + df.count()}... for table {tbl}')\n",
    "        df.write.mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", target_url) \\\n",
    "        .option(\"user\", uid) \\\n",
    "        .option(\"password\", pwd) \\\n",
    "        .option(\"driver\", target_driver) \\\n",
    "        .option(\"dbtable\", \"src_\" + tbl) \\\n",
    "        .save()\n",
    "        print(\"Data imported successful\")\n",
    "        rows_imported += df.count()\n",
    "    except Exception as e:\n",
    "        print(\"Data load error: \" + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
